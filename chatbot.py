# chatbot.py (Final Version with Sidebar Upload, Conditional RAG, and Cache Fix)

import streamlit as st
import os
import logging
import sys
import tempfile # C·∫ßn thi·∫øt ƒë·ªÉ x·ª≠ l√Ω file t·∫£i l√™n
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai # Import ƒë·ªÉ g·ªçi tr·ª±c ti·∫øp khi kh√¥ng RAG
import chromadb
import traceback # ƒê·ªÉ in traceback chi ti·∫øt khi c√≥ l·ªói

# --- 1. C·∫•u h√¨nh ban ƒë·∫ßu v√† API Key ---
# L·ªánh n√†y PH·∫¢I l√† l·ªánh Streamlit ƒë·∫ßu ti√™n
st.set_page_config(page_title="Chatbot Th√¥ng Minh (Gemini + RAG)", layout="wide")

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t. H√£y t·∫°o file .env v√† th√™m GEMINI_API_KEY='YOUR_API_KEY'")
    st.stop()

# C·∫•u h√¨nh API cho th∆∞ vi·ªán google.generativeai (d√πng khi chat th√¥ng th∆∞·ªùng)
try:
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
     st.error(f"L·ªói khi c·∫•u h√¨nh google.generativeai v·ªõi API Key: {e}. Ki·ªÉm tra l·∫°i API Key.")
     st.stop()


logging.basicConfig(stream=sys.stdout, level=logging.INFO) # INFO ho·∫∑c DEBUG

# --- 2. Import c√°c th∆∞ vi·ªán LlamaIndex ---
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    SummaryIndex,         # V·∫´n gi·ªØ Summary Index n·∫øu mu·ªën
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.gemini import Gemini               # LLM cho LlamaIndex
from llama_index.embeddings.gemini import GeminiEmbedding # Embedding cho LlamaIndex
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector
from llama_index.vector_stores.chroma import ChromaVectorStore # Vector store ChromaDB

# --- 3. Kh·ªüi t·∫°o M√¥ h√¨nh (Cache Resource) ---
@st.cache_resource
def initialize_llm_and_embedding():
    """Kh·ªüi t·∫°o v√† cache LLM, Embedding Model cho LlamaIndex Settings v√† GenAI Model."""
    print("--- Kh·ªüi t·∫°o (ho·∫∑c l·∫•y t·ª´ cache) m√¥ h√¨nh LLM v√† Embedding ---")
    genai_chat_model_instance = None
    try:
        # LlamaIndex Settings
        Settings.llm = Gemini(api_key=GEMINI_API_KEY, model_name="models/gemini-1.5-flash-latest")
        Settings.embed_model = GeminiEmbedding(api_key=GEMINI_API_KEY, model_name="models/embedding-001")

        # GenAI Model cho chat th√¥ng th∆∞·ªùng
        genai_chat_model_instance = genai.GenerativeModel('gemini-1.5-flash-latest')

        print("--- ƒê√£ kh·ªüi t·∫°o/l·∫•y m√¥ h√¨nh LlamaIndex Settings v√† GenAI Model---")
        return True, genai_chat_model_instance
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o model Gemini: {e}. Ki·ªÉm tra API Key, t√™n model v√† k·∫øt n·ªëi m·∫°ng.")
        print(f"Model Initialization Error: {e}")
        traceback.print_exc()
        # ƒê·∫£m b·∫£o Settings ƒë∆∞·ª£c reset n·∫øu l·ªói
        Settings.llm = None
        Settings.embed_model = None
        return False, None

# --- 4. Kh·ªüi t·∫°o ChromaDB Client (Cache Resource) ---
@st.cache_resource
def initialize_chroma_client(persist_dir="./chroma_db"):
    """Kh·ªüi t·∫°o v√† cache ChromaDB client."""
    print(f"--- Kh·ªüi t·∫°o (ho·∫∑c l·∫•y t·ª´ cache) ChromaDB client t·∫°i: {persist_dir} ---")
    try:
        os.makedirs(persist_dir, exist_ok=True) # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        db = chromadb.PersistentClient(path=persist_dir)
        print("--- Kh·ªüi t·∫°o ChromaDB client th√†nh c√¥ng ---")
        return db
    except Exception as e:
        st.error(f"L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o ChromaDB client t·∫°i '{persist_dir}': {e}")
        print(f"ChromaDB Client Initialization Error: {e}")
        traceback.print_exc()
        st.stop() # D·ª´ng n·∫øu kh√¥ng th·ªÉ kh·ªüi t·∫°o DB
        return None

# --- 5. H√†m x·ª≠ l√Ω file t·∫£i l√™n v√† t·∫°o/c·∫≠p nh·∫≠t Index ---
def process_uploaded_files(uploaded_files, chroma_db_client, collection_name="rag_gemini_collection"):
    """X·ª≠ l√Ω PDF t·∫£i l√™n, t·∫°o node, th√™m v√†o ChromaDB. Tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng."""
    if not uploaded_files:
        st.warning("Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ x·ª≠ l√Ω.")
        return False
    if not Settings.embed_model:
         st.error("L·ªói: Embedding model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng. Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu.")
         return False

    # S·ª≠ d·ª•ng th∆∞ m·ª•c t·∫°m th·ªùi ƒë·ªÉ l∆∞u file t·∫£i l√™n
    with tempfile.TemporaryDirectory() as temp_dir:
        st.info(f"ƒêang x·ª≠ l√Ω {len(uploaded_files)} file PDF...")
        print(f"--- S·ª≠ d·ª•ng th∆∞ m·ª•c t·∫°m: {temp_dir} ---")
        try:
            # 1. L∆∞u v√† ƒê·ªçc t√†i li·ªáu
            print(f"--- ƒêang ƒë·ªçc t√†i li·ªáu PDF t·ª´ th∆∞ m·ª•c t·∫°m ---")
            reader = SimpleDirectoryReader(input_dir=temp_dir, input_files=[os.path.join(temp_dir, f.name) for f in uploaded_files], required_exts=[".pdf"])
            # L∆∞u file v√†o th∆∞ m·ª•c t·∫°m TR∆Ø·ªöC KHI ƒë·ªçc
            for uploaded_file in uploaded_files:
                temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(temp_file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                print(f"ƒê√£ l∆∞u file t·∫°m: {temp_file_path}")

            documents = reader.load_data(show_progress=True)
            if not documents:
                st.warning("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung t·ª´ c√°c file PDF ƒë√£ t·∫£i l√™n.")
                return False

            # 2. Parse th√†nh Nodes v√† th√™m metadata
            print(f"--- ƒê√£ t·∫£i {len(documents)} document objects. Parse nodes... ---")
            node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
            nodes = node_parser.get_nodes_from_documents(documents, show_progress=True)
            print(f"--- ƒê√£ t·∫°o {len(nodes)} nodes ---")
            if not nodes:
                 st.warning("Kh√¥ng t·∫°o ƒë∆∞·ª£c node n√†o t·ª´ t√†i li·ªáu.")
                 return False

            # Th√™m metadata (v√≠ d·ª•)
            for node in nodes:
                 if 'file_path' in node.metadata:
                      node.metadata['file_type'] = 'pdf'

            # 3. K·∫øt n·ªëi t·ªõi ChromaDB Collection
            print(f"--- L·∫•y ho·∫∑c t·∫°o Chroma collection: {collection_name} ---")
            chroma_collection = chroma_db_client.get_or_create_collection(collection_name)
            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)

            # 4. T·∫°o ho·∫∑c C·∫≠p nh·∫≠t VectorStoreIndex
            print(f"--- ƒêang th√™m {len(nodes)} nodes v√†o Vector Index (ChromaDB) ---")
            VectorStoreIndex(
                nodes,
                storage_context=storage_context,
                embed_model=Settings.embed_model, # S·ª≠ d·ª•ng embed model ƒë√£ kh·ªüi t·∫°o
                show_progress=True
            )
            print(f"--- ƒê√£ c·∫≠p nh·∫≠t th√†nh c√¥ng Vector Index v√†o Chroma collection: {collection_name} ({chroma_collection.count()} m·ª•c) ---")
            st.success(f"ƒê√£ x·ª≠ l√Ω v√† th√™m n·ªôi dung t·ª´ {len(uploaded_files)} file v√†o c∆° s·ªü tri th·ª©c!")
            return True # X·ª≠ l√Ω th√†nh c√¥ng

        except Exception as e:
            st.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω file: {e}")
            print(f"File Processing Error: {e}")
            traceback.print_exc() # In traceback chi ti·∫øt v√†o console
            return False # X·ª≠ l√Ω th·∫•t b·∫°i
        # Th∆∞ m·ª•c t·∫°m s·∫Ω t·ª± ƒë·ªông b·ªã x√≥a

# --- 6. H√†m t·∫£i Index v√† t·∫°o Query Engine (Cache Resource - ƒê√É S·ª¨A L·ªñI HASH) ---
@st.cache_resource
# Th√™m d·∫•u g·∫°ch d∆∞·ªõi (_) v√†o tr∆∞·ªõc chroma_db_client ƒë·ªÉ kh√¥ng hash tham s·ªë n√†y
def load_indexes_and_get_query_engine(_chroma_db_client, collection_name="rag_gemini_collection", summary_persist_dir="./storage_summary"):
    """
    T·∫£i Vector Index t·ª´ Chroma, Summary Index t·ª´ disk (n·∫øu c√≥)
    v√† t·∫°o Router Query Engine.
    L∆ØU √ù: _chroma_db_client ƒë∆∞·ª£c b·ªè qua khi hashing cho cache.
    """
    vector_index = None
    summary_index = None
    query_engine = None
    print("--- T·∫£i (ho·∫∑c l·∫•y t·ª´ cache) Index v√† t·∫°o Query Engine ---")

    # ƒê·∫£m b·∫£o c√°c model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
    if not Settings.llm or not Settings.embed_model:
         print("--- L·ªói: LLM ho·∫∑c Embedding model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o trong Settings. Kh√¥ng th·ªÉ t·∫£i index ho·∫∑c t·∫°o engine. ---")
         # Kh√¥ng hi·ªÉn th·ªã st.error ·ªü ƒë√¢y v√¨ h√†m n√†y ch·∫°y trong cache
         return None

    # 1. T·∫£i Vector Index t·ª´ ChromaDB
    try:
        print(f"--- Ki·ªÉm tra Chroma collection: {collection_name} b·∫±ng client ƒë√£ cung c·∫•p ---")
        # S·ª≠ d·ª•ng _chroma_db_client (tham s·ªë v·ªõi d·∫•u g·∫°ch d∆∞·ªõi)
        chroma_collection = _chroma_db_client.get_collection(collection_name)
        if chroma_collection.count() == 0:
             print(f"--- Collection '{collection_name}' t·ªìn t·∫°i nh∆∞ng r·ªóng. Kh√¥ng th·ªÉ t·∫°o Vector Index. ---")
             vector_index = None
        else:
            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            print(f"--- Collection '{collection_name}' c√≥ {chroma_collection.count()} documents. ƒêang t·∫£i Vector Index... ---")
            vector_index = VectorStoreIndex.from_vector_store(
                vector_store,
                embed_model=Settings.embed_model # C·∫ßn embed_model ƒë·ªÉ load
            )
            print("--- ƒê√£ t·∫£i Vector Index t·ª´ Chroma ---")
    except Exception as e:
        # L·ªói ph·ªï bi·∫øn l√† collection kh√¥ng t·ªìn t·∫°i (ch∆∞a c√≥ file n√†o ƒë∆∞·ª£c x·ª≠ l√Ω)
        print(f"Th√¥ng b√°o: Kh√¥ng th·ªÉ t·∫£i Vector Index t·ª´ Chroma Collection '{collection_name}'. C√≥ th·ªÉ n√≥ ch∆∞a ƒë∆∞·ª£c t·∫°o ho·∫∑c c√≥ l·ªói kh√°c. L·ªói: {e}")
        vector_index = None

    # 2. T·∫£i Summary Index t·ª´ Disk (T√πy ch·ªçn, gi·ªØ l·∫°i n·∫øu b·∫°n mu·ªën d√πng)
    summary_storage_path = Path(summary_persist_dir)
    if summary_storage_path.exists() and summary_storage_path.is_dir():
        try:
            print(f"--- T·∫£i Summary Index t·ª´ {summary_persist_dir} ---")
            summary_storage_context = StorageContext.from_defaults(persist_dir=str(summary_storage_path))
            summary_index = load_index_from_storage(summary_storage_context)
            print("--- ƒê√£ t·∫£i Summary Index ---")
        except Exception as e:
            print(f"L·ªói khi t·∫£i Summary Index t·ª´ '{summary_persist_dir}': {e}. B·ªè qua Summary Index.")
            summary_index = None
    else:
        print(f"--- Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c Summary Index t·∫°i '{summary_persist_dir}'. B·ªè qua. ---")
        summary_index = None

    # 3. T·∫°o Query Engine Tools v√† Router
    query_engine_tools = []
    try:
        if vector_index:
            print("--- T·∫°o Vector Query Engine ---")
            vector_query_engine = vector_index.as_query_engine(similarity_top_k=3, llm=Settings.llm) # Truy·ªÅn LLM v√†o engine
            vector_tool = QueryEngineTool.from_defaults(
                query_engine=vector_query_engine,
                name="vector_search_tool",
                description="H·ªØu √≠ch ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin C·ª§ TH·ªÇ, chi ti·∫øt, ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp t·ª´ n·ªôi dung c√°c t√†i li·ªáu ƒê√É ƒê∆Ø·ª¢C T·∫¢I L√äN.",
            )
            query_engine_tools.append(vector_tool)
        else:
            print("--- B·ªè qua Vector Query Engine (kh√¥ng c√≥ Vector Index) ---")

        if summary_index:
            print("--- T·∫°o Summary Query Engine ---")
            summary_query_engine = summary_index.as_query_engine(
                response_mode="tree_summarize", use_async=True, llm=Settings.llm # Truy·ªÅn LLM v√†o engine
            )
            summary_tool = QueryEngineTool.from_defaults(
                query_engine=summary_query_engine,
                name="summary_tool",
                description="H·ªØu √≠ch ƒë·ªÉ T√ìM T·∫ÆT n·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu (d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥ cho Summary Index).",
            )
            query_engine_tools.append(summary_tool)
        else:
             print("--- B·ªè qua Summary Query Engine (kh√¥ng c√≥ Summary Index) ---")

        # Ch·ªâ t·∫°o Router n·∫øu c√≥ √≠t nh·∫•t 1 tool
        if query_engine_tools:
            print(f"--- T·∫°o Router Query Engine v·ªõi {len(query_engine_tools)} tool(s) ---")
            # Settings.llm ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra ·ªü ƒë·∫ßu h√†m
            query_engine = RouterQueryEngine(
                selector=LLMSingleSelector.from_defaults(llm=Settings.llm),
                query_engine_tools=query_engine_tools,
                verbose=False # B·∫≠t True ƒë·ªÉ debug router selection trong console
            )
            print("--- ƒê√£ t·∫°o Router Query Engine ---")
            return query_engine
        else:
            print("--- Kh√¥ng c√≥ tool n√†o ƒë∆∞·ª£c t·∫°o, kh√¥ng th·ªÉ t·∫°o Router Query Engine ---")
            return None

    except Exception as e:
        # Kh√¥ng n√™n hi·ªÉn th·ªã st.error ·ªü ƒë√¢y v√¨ ch·∫°y trong cache
        print(f"L·ªói khi t·∫°o Query Engine ho·∫∑c Router: {e}")
        traceback.print_exc()
        return None

# --- 7. H√†m g·ªçi Gemini th√¥ng th∆∞·ªùng ---
def get_general_gemini_response(user_prompt, chat_model, chat_history):
    """G·ª≠i prompt ƒë·∫øn Gemini API th√¥ng th∆∞·ªùng v√† tr·∫£ v·ªÅ ph·∫£n h·ªìi text."""
    if not chat_model:
        print("L·ªói: M√¥ h√¨nh chat genai ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")
        return "L·ªói: M√¥ h√¨nh chat ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o."
    try:
        print("--- G·ªçi API Gemini th√¥ng th∆∞·ªùng ---")
        # Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat sang ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
        api_history = []
        for msg in chat_history:
             role = 'user' if msg['role'] == 'user' else 'model'
             # ƒê·∫£m b·∫£o content l√† string
             content = str(msg.get('content', ''))
             api_history.append({'role': role, 'parts': [content]})

        # B·∫Øt ƒë·∫ßu phi√™n chat m·ªõi m·ªói l·∫ßn (ho·∫∑c s·ª≠ d·ª•ng state n·∫øu mu·ªën)
        convo = chat_model.start_chat(history=api_history)
        response = convo.send_message(user_prompt)
        print("--- Nh·∫≠n ph·∫£n h·ªìi t·ª´ Gemini API ---")
        return response.text
    except Exception as e:
        print(f"L·ªói khi g·ªçi Gemini API: {e}")
        traceback.print_exc()
        # Cung c·∫•p th√¥ng tin l·ªói r√µ r√†ng h∆°n cho ng∆∞·ªùi d√πng
        error_detail = str(e)
        if "API key not valid" in error_detail:
             return "L·ªói: API Key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i."
        elif "quota" in error_detail.lower():
             return "L·ªói: ƒê√£ h·∫øt h·∫°n ng·∫°ch s·ª≠ d·ª•ng API Gemini. Vui l√≤ng th·ª≠ l·∫°i sau."
        else:
             return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi k·∫øt n·ªëi t·ªõi Gemini: {e}"


# --- 8. Kh·ªüi t·∫°o ·ª©ng d·ª•ng Streamlit ---
print("--- === B·∫Øt ƒë·∫ßu Kh·ªüi t·∫°o ·ª®ng d·ª•ng Streamlit === ---")

# Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn ch√≠nh ƒë∆∞·ª£c cache
models_ok, genai_chat_model = initialize_llm_and_embedding()
chroma_client = initialize_chroma_client()

# Kh·ªüi t·∫°o session state n·∫øu ch∆∞a c√≥
# D√πng .setdefault() ƒë·ªÉ g·ªçn h∆°n
st.session_state.setdefault("messages", [{"role": "assistant", "content": "Ch√†o b·∫°n! T·∫£i l√™n t√†i li·ªáu PDF ·ªü sidebar ho·∫∑c h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨."}])
st.session_state.setdefault("rag_initialized", False)
st.session_state.setdefault("query_engine", None)
st.session_state.setdefault("engine_loaded", False)
st.session_state.setdefault("processed_files_hash", None) # ƒê·ªÉ theo d√µi file ƒë√£ x·ª≠ l√Ω

# T·∫£i query engine M·ªòT L·∫¶N khi kh·ªüi t·∫°o ho·∫∑c khi c√≥ s·ª± thay ƒë·ªïi c·∫ßn thi·∫øt
# (V√≠ d·ª•: sau khi x·ª≠ l√Ω file)
if models_ok and chroma_client and not st.session_state.engine_loaded:
    print("--- T·∫£i/L√†m m·ªõi Query Engine ---")
    # Truy·ªÅn client ƒë√£ kh·ªüi t·∫°o v√†o h√†m cache (v·ªõi d·∫•u g·∫°ch d∆∞·ªõi trong ƒë·ªãnh nghƒ©a h√†m)
    st.session_state.query_engine = load_indexes_and_get_query_engine(chroma_client)
    st.session_state.engine_loaded = True # ƒê√°nh d·∫•u ƒë√£ t·∫£i xong engine l·∫ßn n√†y

    # Ki·ªÉm tra tr·∫°ng th√°i ban ƒë·∫ßu c·ªßa RAG d·ª±a tr√™n ChromaDB
    try:
        collection = chroma_client.get_collection("rag_gemini_collection")
        if collection.count() > 0:
            st.session_state.rag_initialized = True
            print(f"--- ƒê√£ ph√°t hi·ªán RAG Index t·ªìn t·∫°i ({collection.count()} m·ª•c). Ch·∫ø ƒë·ªô RAG ban ƒë·∫ßu: ON ---")
        else:
             st.session_state.rag_initialized = False
             print("--- Kh√¥ng ph√°t hi·ªán d·ªØ li·ªáu trong RAG Index. Ch·∫ø ƒë·ªô RAG ban ƒë·∫ßu: OFF ---")
    except Exception as e: # Collection c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i
        st.session_state.rag_initialized = False
        print(f"--- Collection RAG ch∆∞a t·ªìn t·∫°i ho·∫∑c l·ªói khi ki·ªÉm tra: {e}. Ch·∫ø ƒë·ªô RAG ban ƒë·∫ßu: OFF ---")

# --- 9. Giao di·ªán Streamlit ---
st.title("üìö Chatbot Th√¥ng Minh (Gemini + LlamaIndex + ChromaDB)")

# --- Sidebar ---
with st.sidebar:
    st.title("üìÅ Qu·∫£n l√Ω T√†i li·ªáu")
    st.markdown("T·∫£i l√™n c√°c file PDF c·ªßa b·∫°n t·∫°i ƒë√¢y ƒë·ªÉ chatbot tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung ƒë√≥.")

    uploaded_files = st.file_uploader(
        "Ch·ªçn file PDF",
        accept_multiple_files=True,
        type=["pdf"],
        key="file_uploader" # Th√™m key ƒë·ªÉ qu·∫£n l√Ω state t·ªët h∆°n
    )

    if st.button("X·ª≠ l√Ω T√†i li·ªáu ƒë√£ t·∫£i l√™n", key="process_button"):
        if uploaded_files:
            # T√≠nh hash c·ªßa danh s√°ch file ƒë·ªÉ ki·ªÉm tra thay ƒë·ªïi
            current_files_hash = hash(tuple(sorted(f.name for f in uploaded_files)))

            # Ch·ªâ x·ª≠ l√Ω n·∫øu file thay ƒë·ªïi ho·∫∑c ch∆∞a x·ª≠ l√Ω l·∫ßn n√†o
            # (Gi√∫p tr√°nh x·ª≠ l√Ω l·∫°i c√πng file nhi·ªÅu l·∫ßn n·∫øu kh√¥ng c·∫ßn)
            # if current_files_hash != st.session_state.processed_files_hash: -> Logic n√†y c√≥ th·ªÉ ph·ª©c t·∫°p, t·∫°m th·ªùi b·ªè qua, x·ª≠ l√Ω m·ªói l·∫ßn nh·∫•n n√∫t
            print("--- Nh·∫•n n√∫t X·ª≠ l√Ω T√†i li·ªáu ---")
            if models_ok and chroma_client:
                success = process_uploaded_files(uploaded_files, chroma_client)
                if success:
                    st.session_state.rag_initialized = True
                    st.session_state.engine_loaded = False # Bu·ªôc load l·∫°i engine
                    # st.session_state.processed_files_hash = current_files_hash # L∆∞u hash file ƒë√£ x·ª≠ l√Ω
                    st.success("X·ª≠ l√Ω ho√†n t·∫•t! H·ªá th·ªëng ƒë√£ s·∫µn s√†ng cho RAG.")
                    st.rerun() # Ch·∫°y l·∫°i ƒë·ªÉ load engine m·ªõi v√† c·∫≠p nh·∫≠t UI
                else:
                    st.error("X·ª≠ l√Ω t√†i li·ªáu th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra log l·ªói trong console.")
            else:
                 st.error("L·ªói kh·ªüi t·∫°o Model ho·∫∑c ChromaDB, kh√¥ng th·ªÉ x·ª≠ l√Ω file.")
            # else:
            #     st.info("C√°c file n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.")
        else:
            st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt file PDF ƒë·ªÉ x·ª≠ l√Ω.")

    st.divider()
    # Hi·ªÉn th·ªã tr·∫°ng th√°i RAG r√µ r√†ng h∆°n
    rag_status = st.session_state.get("rag_initialized", False)
    status_text = "S·∫µn s√†ng (d·ª±a tr√™n t√†i li·ªáu ƒë√£ x·ª≠ l√Ω)" if rag_status else "Ch∆∞a s·∫µn s√†ng (h·ªèi ƒë√°p th√¥ng th∆∞·ªùng)"
    status_icon = "‚úÖ" if rag_status else "‚ÑπÔ∏è"
    st.markdown(f"**{status_icon} Tr·∫°ng th√°i RAG:** {status_text}")
    if rag_status and not st.session_state.get("query_engine"):
         st.warning("RAG ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t nh∆∞ng Query Engine ch∆∞a t·∫£i ƒë∆∞·ª£c. C√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω l·∫°i file ho·∫∑c ki·ªÉm tra l·ªói.")


# --- Khu v·ª±c Chat ch√≠nh ---
print(f"--- Tr·∫°ng th√°i RAG hi·ªán t·∫°i: {st.session_state.rag_initialized}, Query Engine: {'C√≥' if st.session_state.query_engine else 'Kh√¥ng'} ---")

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
container = st.container() # T·∫°o container ƒë·ªÉ chat messages kh√¥ng b·ªã render l·∫°i lung tung
with container:
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

# Input c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", key="chat_input"):
    if not models_ok:
         st.error("H·ªá th·ªëng ch∆∞a s·∫µn s√†ng do l·ªói kh·ªüi t·∫°o model.")
         st.stop()

    # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã ngay l·∫≠p t·ª©c
    st.session_state.messages.append({"role": "user", "content": prompt})
    with container: # Hi·ªÉn th·ªã l·∫°i trong container
         st.chat_message("user").write(prompt)

    # T·∫°o placeholder cho ph·∫£n h·ªìi c·ªßa assistant
    response_placeholder = container.chat_message("assistant").empty()
    response_text = ""

    # Quy·∫øt ƒë·ªãnh d√πng RAG hay chat th√¥ng th∆∞·ªùng
    use_rag = st.session_state.get("rag_initialized", False)
    current_query_engine = st.session_state.get("query_engine", None)

    if use_rag and current_query_engine:
        print(f"--- S·ª≠ d·ª•ng RAG Query Engine ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: '{prompt[:50]}...' ---")
        try:
            with st.spinner("üß† ƒêang t∆∞ duy d·ª±a tr√™n t√†i li·ªáu..."):
                response = current_query_engine.query(prompt)
                response_text = str(response)

            response_placeholder.write(response_text)
            st.session_state.messages.append({"role": "assistant", "content": response_text})

            # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o (n·∫øu c√≥)
            if response_text and hasattr(response, 'source_nodes') and response.source_nodes:
                 with st.expander("Ngu·ªìn t√†i li·ªáu tham kh·∫£o (t·ª´ RAG)"):
                     for i, snode in enumerate(response.source_nodes):
                        metadata = snode.metadata
                        # C·ªë g·∫Øng l·∫•y t√™n file g·ªëc t·ª´ metadata n·∫øu SimpleDirectoryReader th√™m v√†o
                        file_name = metadata.get('file_name', metadata.get('file_path', 'N/A'))
                        display_name = os.path.basename(file_name)
                        file_type = metadata.get('file_type', 'pdf')
                        relevance_score = snode.score

                        st.markdown(f"**Ngu·ªìn {i+1}:**")
                        st.markdown(f"- **File:** `{display_name}` ({file_type})")
                        if relevance_score is not None:
                            st.markdown(f"- **ƒê·ªô li√™n quan (Score):** `{relevance_score:.4f}`")
                        else:
                            st.markdown(f"- **ƒê·ªô li√™n quan (Score):** `N/A`")
                        # Th√™m t√πy ch·ªçn hi·ªÉn th·ªã n·ªôi dung node (h·ªØu √≠ch khi debug)
                        # with st.popover("Xem tr√≠ch d·∫´n"):
                        #      st.markdown(f"```\n{snode.get_content()}\n```")
                        st.divider()

        except Exception as e:
            error_message = f"R·∫•t ti·∫øc, ƒë√£ x·∫£y ra l·ªói khi truy v·∫•n RAG: {e}"
            print(f"RAG Query Error: {e}")
            traceback.print_exc()
            response_placeholder.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": f"L·ªói RAG: {error_message}"}) # Th√™m l·ªói v√†o history

    elif use_rag and not current_query_engine:
         warning_message = "Ch·∫ø ƒë·ªô RAG ƒëang b·∫≠t nh∆∞ng Query Engine ch∆∞a s·∫µn s√†ng. C√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω l·∫°i file ho·∫∑c ki·ªÉm tra l·ªói kh·ªüi t·∫°o."
         print(warning_message)
         response_placeholder.warning(warning_message)
         st.session_state.messages.append({"role": "assistant", "content": warning_message}) # Th√¥ng b√°o cho user

    else: # Kh√¥ng d√πng RAG -> Chat th√¥ng th∆∞·ªùng
        print(f"--- S·ª≠ d·ª•ng Gemini th√¥ng th∆∞·ªùng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: '{prompt[:50]}...' ---")
        if genai_chat_model:
            with st.spinner("üí¨ ƒêang tr√≤ chuy·ªán v·ªõi Gemini..."):
                 # L·∫•y l·ªãch s·ª≠ chat hi·ªán t·∫°i (b·ªè c√¢u h·ªèi user cu·ªëi c√πng)
                 chat_history_for_api = st.session_state.messages[:-1]
                 response_text = get_general_gemini_response(prompt, genai_chat_model, chat_history_for_api)

            response_placeholder.write(response_text)
            st.session_state.messages.append({"role": "assistant", "content": response_text})
        else:
             error_message = "L·ªói: M√¥ h√¨nh chat th√¥ng th∆∞·ªùng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o."
             print(error_message)
             response_placeholder.error(error_message)
             st.session_state.messages.append({"role": "assistant", "content": error_message})
