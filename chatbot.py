import streamlit as st
import os
import logging
import sys
from dotenv import load_dotenv

st.set_page_config(page_title="H·ªá th·ªëng RAG v·ªõi Gemini", layout="wide")
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    st.error(
        "API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t. H√£y t·∫°o file .env v√† th√™m GEMINI_API_KEY='YOUR_API_KEY'"
    )
    st.stop()
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    SummaryIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector


@st.cache_resource
def initialize_models():
    print("--- Kh·ªüi t·∫°o (ho·∫∑c l·∫•y t·ª´ cache) m√¥ h√¨nh LLM v√† Embedding ---")
    try:
        Settings.llm = Gemini(
            api_key=GEMINI_API_KEY, model_name="models/gemini-1.5-pro-latest"
        )
        Settings.embed_model = GeminiEmbedding(
            api_key=GEMINI_API_KEY, model_name="models/embedding-001"
        )
        print("--- ƒê√£ kh·ªüi t·∫°o/l·∫•y m√¥ h√¨nh ---")
        return True
    except Exception as e:
        st.error(
            f"L·ªói khi kh·ªüi t·∫°o model Gemini: {e}. Ki·ªÉm tra API Key v√† k·∫øt n·ªëi m·∫°ng."
        )
        return False


@st.cache_resource
def load_and_index_data(
    data_dir="data",
    persist_dir_vector="./storage_vector",
    persist_dir_summary="./storage_summary",
):
    vector_index = None
    summary_index = None
    print(f"--- B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫£i/t·∫°o index (cache resource) ---")
    vector_storage_exists = os.path.exists(persist_dir_vector)
    summary_storage_exists = os.path.exists(persist_dir_summary)
    if vector_storage_exists:
        try:
            print(f"--- T·∫£i Vector Index t·ª´ {persist_dir_vector} (cache) ---")
            storage_context_vector = StorageContext.from_defaults(
                persist_dir=persist_dir_vector
            )
            vector_index = load_index_from_storage(storage_context_vector)
            print("--- ƒê√£ t·∫£i Vector Index ---")
        except Exception as e:
            print(f"L·ªói khi t·∫£i Vector Index t·ª´ storage: {e}. S·∫Ω th·ª≠ t·∫°o l·∫°i.")
            vector_storage_exists = False
            vector_index = None
    if summary_storage_exists:
        try:
            print(f"--- T·∫£i Summary Index t·ª´ {persist_dir_summary} (cache) ---")
            storage_context_summary = StorageContext.from_defaults(
                persist_dir=persist_dir_summary
            )
            summary_index = load_index_from_storage(storage_context_summary)
            print("--- ƒê√£ t·∫£i Summary Index ---")
        except Exception as e:
            print(f"L·ªói khi t·∫£i Summary Index t·ª´ storage: {e}. S·∫Ω th·ª≠ t·∫°o l·∫°i.")
            summary_storage_exists = False
            summary_index = None
    if vector_index is None or summary_index is None:
        print(
            "--- Index ch∆∞a t·ªìn t·∫°i, kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c t·∫£i l·ªói -> T·∫£i d·ªØ li·ªáu v√† t·∫°o l·∫°i index ---"
        )
        if not os.path.exists(data_dir):
            st.error(
                f"Th∆∞ m·ª•c d·ªØ li·ªáu '{data_dir}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng t·∫°o v√† ƒë·∫∑t file v√†o."
            )
            st.stop()
        with st.spinner(
            f"ƒêang t·∫£i v√† x·ª≠ l√Ω t√†i li·ªáu t·ª´ '{data_dir}'... Vi·ªác n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t l·∫ßn ƒë·∫ßu."
        ):
            try:
                print(f"--- ƒêang ƒë·ªçc t√†i li·ªáu t·ª´: {data_dir} ---")
                reader = SimpleDirectoryReader(input_dir=data_dir, recursive=True)
                documents = reader.load_data()
                if not documents:
                    st.warning(
                        f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong '{data_dir}'. H·ªá th·ªëng s·∫Ω kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ truy v·∫•n."
                    )
                    return None, None
                print(
                    f"--- ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu. B·∫Øt ƒë·∫ßu th√™m metadata v√† parse nodes ---"
                )
                for doc in documents:
                    file_path = doc.metadata.get("file_path")
                    if file_path:
                        file_extension = os.path.splitext(file_path)[1].lower()
                        doc.metadata["file_type"] = file_extension.replace(".", "")
                    else:
                        doc.metadata["file_type"] = "unknown"
                node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
                nodes = node_parser.get_nodes_from_documents(
                    documents, show_progress=False
                )
                print(f"--- ƒê√£ t·∫°o {len(nodes)} node. B·∫Øt ƒë·∫ßu t·∫°o index ---")
                if vector_index is None:
                    print("--- T·∫°o Vector Index m·ªõi ---")
                    vector_index = VectorStoreIndex(nodes, show_progress=False)
                    print("--- L∆∞u tr·ªØ Vector Index v√†o: {persist_dir_vector} ---")
                    vector_index.storage_context.persist(persist_dir=persist_dir_vector)
                if summary_index is None:
                    print("--- T·∫°o Summary Index m·ªõi ---")
                    summary_index = SummaryIndex(nodes, show_progress=False)
                    print("--- L∆∞u tr·ªØ Summary Index v√†o: {persist_dir_summary} ---")
                    summary_index.storage_context.persist(
                        persist_dir=persist_dir_summary
                    )
            except Exception as e:
                st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i ho·∫∑c t·∫°o index: {e}")
                st.stop()
    print("--- K·∫øt th√∫c qu√° tr√¨nh t·∫£i/t·∫°o index ---")
    return vector_index, summary_index


@st.cache_resource
def get_query_engine(_vector_index, _summary_index):
    print("--- T·∫°o (ho·∫∑c l·∫•y t·ª´ cache) c√°c Query Engine v√† Router ---")
    if _vector_index is None or _summary_index is None:
        st.warning("Index kh√¥ng h·ª£p l·ªá, kh√¥ng th·ªÉ t·∫°o Query Engine.")
        return None
    try:
        vector_query_engine = _vector_index.as_query_engine(similarity_top_k=3)
        summary_query_engine = _summary_index.as_query_engine(
            response_mode="tree_summarize", use_async=True
        )
        vector_tool = QueryEngineTool.from_defaults(
            query_engine=vector_query_engine,
            name="vector_search_tool",
            description=(
                "H·ªØu √≠ch ƒë·ªÉ t√¨m ki·∫øm v√† truy xu·∫•t th√¥ng tin c·ª• th·ªÉ, chi ti·∫øt, ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp t·ª´ n·ªôi dung c√°c t√†i li·ªáu (PDF, TXT, DOCX)."
            ),
        )
        summary_tool = QueryEngineTool.from_defaults(
            query_engine=summary_query_engine,
            name="summary_tool",
            description=(
                "H·ªØu √≠ch ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa to√†n b·ªô ho·∫∑c m·ªôt ph·∫ßn l·ªõn c√°c t√†i li·ªáu."
            ),
        )
        query_engine = RouterQueryEngine(
            selector=LLMSingleSelector.from_defaults(llm=Settings.llm),
            query_engine_tools=[vector_tool, summary_tool],
            verbose=False,
        )
        print("--- ƒê√£ t·∫°o/l·∫•y Router Query Engine ---")
        return query_engine
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o Query Engine ho·∫∑c Router: {e}")
        return None


print("--- B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o ·ª©ng d·ª•ng Streamlit ---")
models_initialized = initialize_models()
if models_initialized:
    vector_index, summary_index = load_and_index_data()
    if vector_index is None or summary_index is None:
        st.warning(
            "Kh√¥ng th·ªÉ t·∫£i ho·∫∑c t·∫°o index. H·ªá th·ªëng c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông ƒë√∫ng."
        )
        query_engine = None
    else:
        query_engine = get_query_engine(vector_index, summary_index)
else:
    st.error(
        "Kh√¥ng th·ªÉ kh·ªüi t·∫°o m√¥ h√¨nh LLM/Embedding. Vui l√≤ng ki·ªÉm tra l·ªói v√† th·ª≠ l·∫°i."
    )
    vector_index = None
    summary_index = None
    query_engine = None
print("--- Ho√†n t·∫•t kh·ªüi t·∫°o ·ª©ng d·ª•ng (tr∆∞·ªõc ph·∫ßn UI) ---")
st.title("üìö H·ªá th·ªëng H·ªèi ƒê√°p T√†i Li·ªáu s·ª≠ d·ª•ng Gemini & LlamaIndex")
st.caption("ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung c√°c file trong th∆∞ m·ª•c 'data' (PDF, TXT, DOCX)")
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ch√†o b·∫°n! B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ c√°c t√†i li·ªáu?"}
    ]
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    if not models_initialized:
        st.error(
            "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng do l·ªói kh·ªüi t·∫°o model. Vui l√≤ng ki·ªÉm tra log v√† c·∫•u h√¨nh."
        )
        st.stop()
    if not query_engine:
        st.error(
            "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng do l·ªói t·∫£i/t·∫°o index ho·∫∑c query engine. Vui l√≤ng ki·ªÉm tra log."
        )
        st.stop()
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    response_placeholder = st.chat_message("assistant").empty()
    try:
        with st.spinner("ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
            response = query_engine.query(prompt)
            response_text = str(response)
        response_placeholder.write(response_text)
        st.session_state.messages.append(
            {"role": "assistant", "content": response_text}
        )
        if (
            response_text
            and hasattr(response, "source_nodes")
            and response.source_nodes
        ):
            with st.expander("Ngu·ªìn t√†i li·ªáu tham kh·∫£o"):
                for i, snode in enumerate(response.source_nodes):
                    metadata = snode.metadata
                    file_name = metadata.get("file_path", "N/A")
                    file_type = metadata.get("file_type", "N/A")
                    relevance_score = snode.score
                    display_name = os.path.basename(file_name)
                    st.markdown(f"**Ngu·ªìn {i+1}:**")
                    st.markdown(f"- **File:** `{display_name}` ({file_type})")
                    st.markdown(f"- **ƒê·ªô li√™n quan (Score):** `{relevance_score:.4f}`")
                    st.divider()
    except Exception as e:
        error_message = f"R·∫•t ti·∫øc, ƒë√£ x·∫£y ra l·ªói trong qu√° tr√¨nh truy v·∫•n: {e}"
        print(f"Query Error: {e}")
        response_placeholder.error(error_message)
        st.session_state.messages.append(
            {"role": "assistant", "content": error_message}
        )
