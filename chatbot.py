import streamlit as st
import os
import logging
import sys
from dotenv import load_dotenv
import google.generativeai as genai
import chromadb

st.set_page_config(page_title="H·ªá th·ªëng RAG v·ªõi Gemini & ChromaDB", layout="wide")
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    st.error(
        "API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t. H√£y t·∫°o file .env v√† th√™m GEMINI_API_KEY='YOUR_API_KEY'"
    )
    st.stop()
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    SummaryIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core.tools import QueryEngineTool
from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector
from llama_index.vector_stores.chroma import ChromaVectorStore


@st.cache_resource
def initialize_models():
    print("--- Kh·ªüi t·∫°o (ho·∫∑c l·∫•y t·ª´ cache) m√¥ h√¨nh LLM v√† Embedding ---")
    try:
        Settings.llm = Gemini(
            api_key=GEMINI_API_KEY, model_name="models/gemini-2.0-flash"
        )
        Settings.embed_model = GeminiEmbedding(
            api_key=GEMINI_API_KEY, model_name="models/embedding-001"
        )
        print("--- ƒê√£ kh·ªüi t·∫°o/l·∫•y m√¥ h√¨nh ---")
        return True
    except Exception as e:
        st.error(
            f"L·ªói khi kh·ªüi t·∫°o model Gemini: {e}. Ki·ªÉm tra API Key, t√™n model v√† k·∫øt n·ªëi m·∫°ng."
        )
        return False


@st.cache_resource
def load_and_index_data(
    data_dir="data",
    chroma_persist_dir="./chroma_db",
    chroma_collection_name="rag_gemini_collection",
    summary_persist_dir="./storage_summary",
):
    vector_index = None
    summary_index = None
    nodes = []
    print(f"--- B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫£i/t·∫°o index (ChromaDB & Summary Index) ---")
    print(f"--- Kh·ªüi t·∫°o ChromaDB client t·∫°i: {chroma_persist_dir} ---")
    try:
        db = chromadb.PersistentClient(path=chroma_persist_dir)
        print(f"--- L·∫•y ho·∫∑c t·∫°o Chroma collection: {chroma_collection_name} ---")
        chroma_collection = db.get_or_create_collection(chroma_collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    except Exception as e:
        st.error(
            f"L·ªói khi kh·ªüi t·∫°o ho·∫∑c k·∫øt n·ªëi t·ªõi ChromaDB t·∫°i '{chroma_persist_dir}': {e}"
        )
        st.stop()
    collection_exists_and_has_data = False
    if os.path.exists(chroma_persist_dir) and chroma_collection.count() > 0:
        print(
            f"--- Collection '{chroma_collection_name}' ƒë√£ t·ªìn t·∫°i v√† c√≥ {chroma_collection.count()} documents. ƒêang t·∫£i Vector Index t·ª´ Chroma. ---"
        )
        collection_exists_and_has_data = True
        vector_index = VectorStoreIndex.from_vector_store(
            vector_store, embed_model=Settings.embed_model
        )
    else:
        print(
            f"--- Collection '{chroma_collection_name}' ch∆∞a c√≥ d·ªØ li·ªáu ho·∫∑c th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i. S·∫Ω x·ª≠ l√Ω t√†i li·ªáu v√† t·∫°o index m·ªõi. ---"
        )
    if not collection_exists_and_has_data:
        print("--- T·∫£i d·ªØ li·ªáu v√† t·∫°o Vector Index m·ªõi v√†o ChromaDB ---")
        if not os.path.exists(data_dir):
            st.error(f"Th∆∞ m·ª•c d·ªØ li·ªáu '{data_dir}' kh√¥ng t·ªìn t·∫°i.")
            st.stop()
        with st.spinner(
            f"ƒêang t·∫£i, x·ª≠ l√Ω t√†i li·ªáu t·ª´ '{data_dir}' v√† t·∫°o index v√†o ChromaDB..."
        ):
            try:
                print(f"--- ƒêang ƒë·ªçc t√†i li·ªáu t·ª´: {data_dir} ---")
                reader = SimpleDirectoryReader(input_dir=data_dir, recursive=True)
                documents = reader.load_data()
                if not documents:
                    st.warning(f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu trong '{data_dir}'.")
                    return None, None
                print(f"--- ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu. Parse nodes... ---")
                for doc in documents:
                    file_path = doc.metadata.get("file_path")
                    if file_path:
                        file_extension = os.path.splitext(file_path)[1].lower()
                        doc.metadata["file_type"] = file_extension.replace(".", "")
                    else:
                        doc.metadata["file_type"] = "unknown"
                node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
                nodes = node_parser.get_nodes_from_documents(
                    documents, show_progress=False
                )
                print(
                    f"--- ƒê√£ t·∫°o {len(nodes)} node. B·∫Øt ƒë·∫ßu t·∫°o Vector Index v√†o Chroma ---"
                )
                storage_context = StorageContext.from_defaults(
                    vector_store=vector_store
                )
                vector_index = VectorStoreIndex(
                    nodes,
                    storage_context=storage_context,
                    embed_model=Settings.embed_model,
                    show_progress=True,
                )
                print(
                    f"--- ƒê√£ t·∫°o v√† l∆∞u Vector Index v√†o Chroma collection: {chroma_collection_name} ---"
                )
            except Exception as e:
                st.error(
                    f"L·ªói nghi√™m tr·ªçng khi t·∫£i t√†i li·ªáu ho·∫∑c t·∫°o Vector Index v√†o Chroma: {e}"
                )
                st.stop()
    if not nodes:
        if not os.path.exists(data_dir):
            st.warning(
                f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c data '{data_dir}' ƒë·ªÉ t·∫°o Summary Index n·∫øu c·∫ßn."
            )
        else:
            print(
                f"--- ƒê·ªçc l·∫°i t√†i li·ªáu ƒë·ªÉ chu·∫©n b·ªã cho Summary Index (n·∫øu c·∫ßn t·∫°o m·ªõi) ---"
            )
            try:
                reader = SimpleDirectoryReader(input_dir=data_dir, recursive=True)
                documents = reader.load_data()
                if documents:
                    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
                    nodes = node_parser.get_nodes_from_documents(
                        documents, show_progress=False
                    )
                else:
                    print("--- Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªÉ t·∫°o Summary Index ---")
            except Exception as e:
                print(f"L·ªói khi ƒë·ªçc l·∫°i t√†i li·ªáu cho Summary Index: {e}")
    summary_storage_exists = os.path.exists(summary_persist_dir)
    if summary_storage_exists:
        try:
            print(f"--- T·∫£i Summary Index t·ª´ {summary_persist_dir} (cache) ---")
            storage_context_summary = StorageContext.from_defaults(
                persist_dir=summary_persist_dir
            )
            summary_index = load_index_from_storage(storage_context_summary)
            print("--- ƒê√£ t·∫£i Summary Index ---")
        except Exception as e:
            print(f"L·ªói khi t·∫£i Summary Index t·ª´ storage: {e}. S·∫Ω th·ª≠ t·∫°o l·∫°i.")
            summary_storage_exists = False
            summary_index = None
    if not summary_storage_exists and nodes:
        print("--- T·∫°o Summary Index m·ªõi ---")
        try:
            summary_storage_context = StorageContext.from_defaults()
            summary_index = SummaryIndex(
                nodes, storage_context=summary_storage_context, show_progress=True
            )
            print(f"--- L∆∞u tr·ªØ Summary Index v√†o: {summary_persist_dir} ---")
            summary_storage_context.persist(persist_dir=summary_persist_dir)
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o ho·∫∑c l∆∞u Summary Index: {e}")
            summary_index = None
    elif not nodes:
        print("--- Kh√¥ng c√≥ nodes ƒë·ªÉ t·∫°o Summary Index m·ªõi. ---")
        summary_index = None
    print("--- K·∫øt th√∫c qu√° tr√¨nh t·∫£i/t·∫°o index (Chroma & Summary) ---")
    return vector_index, summary_index


@st.cache_resource
def get_query_engine(_vector_index, _summary_index):
    print("--- T·∫°o (ho·∫∑c l·∫•y t·ª´ cache) c√°c Query Engine v√† Router ---")
    query_engine = None
    has_vector_engine = _vector_index is not None
    has_summary_engine = _summary_index is not None
    if not has_vector_engine and not has_summary_engine:
        st.warning(
            "Kh√¥ng c√≥ Vector Index ho·∫∑c Summary Index h·ª£p l·ªá, kh√¥ng th·ªÉ t·∫°o Query Engine."
        )
        return None
    query_engine_tools = []
    try:
        if has_vector_engine:
            print("--- T·∫°o Vector Query Engine ---")
            vector_query_engine = _vector_index.as_query_engine(similarity_top_k=3)
            vector_tool = QueryEngineTool.from_defaults(
                query_engine=vector_query_engine,
                name="vector_search_tool",
                description="H·ªØu √≠ch ƒë·ªÉ t√¨m ki·∫øm v√† truy xu·∫•t th√¥ng tin c·ª• th·ªÉ, chi ti·∫øt, ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp t·ª´ n·ªôi dung c√°c t√†i li·ªáu (PDF, TXT, DOCX).",
            )
            query_engine_tools.append(vector_tool)
        else:
            print(
                "--- B·ªè qua vi·ªác t·∫°o Vector Query Engine do kh√¥ng c√≥ Vector Index ---"
            )
        if has_summary_engine:
            print("--- T·∫°o Summary Query Engine ---")
            summary_query_engine = _summary_index.as_query_engine(
                response_mode="tree_summarize", use_async=True
            )
            summary_tool = QueryEngineTool.from_defaults(
                query_engine=summary_query_engine,
                name="summary_tool",
                description="H·ªØu √≠ch ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa to√†n b·ªô ho·∫∑c m·ªôt ph·∫ßn l·ªõn c√°c t√†i li·ªáu.",
            )
            query_engine_tools.append(summary_tool)
        else:
            print(
                "--- B·ªè qua vi·ªác t·∫°o Summary Query Engine do kh√¥ng c√≥ Summary Index ---"
            )
        if query_engine_tools:
            print("--- T·∫°o Router Query Engine ---")
            query_engine = RouterQueryEngine(
                selector=LLMSingleSelector.from_defaults(llm=Settings.llm),
                query_engine_tools=query_engine_tools,
                verbose=False,
            )
            print("--- ƒê√£ t·∫°o/l·∫•y Router Query Engine ---")
        else:
            st.error("Kh√¥ng c√≥ Query Engine n√†o ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng.")
            return None
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o Query Engine ho·∫∑c Router: {e}")
        return None
    return query_engine


print("--- B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o ·ª©ng d·ª•ng Streamlit v·ªõi ChromaDB ---")
models_initialized = initialize_models()
if models_initialized:
    vector_index, summary_index = load_and_index_data()
    if vector_index is None and summary_index is None:
        st.warning(
            "Kh√¥ng th·ªÉ t·∫£i ho·∫∑c t·∫°o b·∫•t k·ª≥ index n√†o (Vector ho·∫∑c Summary). H·ªá th·ªëng c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông."
        )
        query_engine = None
    else:
        query_engine = get_query_engine(vector_index, summary_index)
else:
    st.error(
        "Kh√¥ng th·ªÉ kh·ªüi t·∫°o m√¥ h√¨nh LLM/Embedding. Vui l√≤ng ki·ªÉm tra l·ªói v√† th·ª≠ l·∫°i."
    )
    query_engine = None
print("--- Ho√†n t·∫•t kh·ªüi t·∫°o ·ª©ng d·ª•ng (tr∆∞·ªõc ph·∫ßn UI) ---")
st.title("üìö H·ªá th·ªëng H·ªèi ƒê√°p T√†i Li·ªáu (Gemini + LlamaIndex + ChromaDB)")
st.caption("ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung c√°c file trong th∆∞ m·ª•c 'data' (PDF, TXT, DOCX)")
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ch√†o b·∫°n! B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ c√°c t√†i li·ªáu?"}
    ]
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    if not models_initialized:
        st.error("H·ªá th·ªëng ch∆∞a s·∫µn s√†ng do l·ªói kh·ªüi t·∫°o model.")
        st.stop()
    if not query_engine:
        st.error(
            "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng do kh√¥ng c√≥ Query Engine n√†o ho·∫°t ƒë·ªông. Ki·ªÉm tra log l·ªói index."
        )
        st.stop()
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    response_placeholder = st.chat_message("assistant").empty()
    try:
        with st.spinner("ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
            response = query_engine.query(prompt)
            response_text = str(response)
        response_placeholder.write(response_text)
        st.session_state.messages.append(
            {"role": "assistant", "content": response_text}
        )
        if (
            response_text
            and hasattr(response, "source_nodes")
            and response.source_nodes
        ):
            with st.expander("Ngu·ªìn t√†i li·ªáu tham kh·∫£o"):
                for i, snode in enumerate(response.source_nodes):
                    metadata = snode.metadata
                    file_name = metadata.get("file_path", "N/A")
                    file_type = metadata.get("file_type", "N/A")
                    relevance_score = snode.score
                    display_name = os.path.basename(file_name)
                    st.markdown(f"**Ngu·ªìn {i+1}:**")
                    st.markdown(f"- **File:** `{display_name}` ({file_type})")
                    if relevance_score is not None:
                        st.markdown(
                            f"- **ƒê·ªô li√™n quan (Score):** `{relevance_score:.4f}`"
                        )
                    else:
                        st.markdown(f"- **ƒê·ªô li√™n quan (Score):** `N/A`")
                    st.divider()
    except Exception as e:
        error_message = f"R·∫•t ti·∫øc, ƒë√£ x·∫£y ra l·ªói trong qu√° tr√¨nh truy v·∫•n: {e}"
        print(f"Query Error Type: {type(e).__name__}, Message: {e}")
        import traceback

        print(traceback.format_exc())
        response_placeholder.error(error_message)
        st.session_state.messages.append(
            {"role": "assistant", "content": error_message}
        )
